{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TYPE = 'validation'\n",
    "TYPE = 'evaluation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging by Concat to Not Lose dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TARGET = 'sales'         \n",
    "if TYPE == 'validmyself':\n",
    "    END_TRAIN = 1913-28 \n",
    "elif TYPE == 'validation':\n",
    "    END_TRAIN = 1913\n",
    "elif TYPE == 'evaluation':\n",
    "    END_TRAIN = 1913+28\n",
    "else:\n",
    "    print('ERROR')   \n",
    "MAIN_INDEX = ['id','d'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Main Data\n"
     ]
    }
   ],
   "source": [
    "# data without any limitations and dtype modification\n",
    "if TYPE == 'validmyself':\n",
    "    train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validmyself.csv')\n",
    "elif TYPE == 'validation':\n",
    "    train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "elif TYPE == 'evaluation':\n",
    "    train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "else:\n",
    "    print('WRONG!!!')\n",
    "    \n",
    "prices_df = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "calendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Grid\n",
      "Train rows: 30490 59181090\n",
      "    Original grid_df:   3.6GiB\n",
      "     Reduced grid_df:   1.3GiB\n"
     ]
    }
   ],
   "source": [
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "grid_df = pd.melt(train_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "'''\n",
    "If we look on train_df, we see that we don't have a great number of rows.\n",
    "However, each day can provide more train data.\n",
    "'''\n",
    "\n",
    "print('Train rows:', len(train_df), len(grid_df))\n",
    "\n",
    "'''\n",
    "To be able to make predictions, we need test set in our grid.\n",
    "'''\n",
    "\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1,29):\n",
    "    temp_df = train_df[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df,add_grid])\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Remove some temoprary DFs\n",
    "del temp_df, add_grid\n",
    "\n",
    "# We don't need original train_df\n",
    "del train_df\n",
    "\n",
    "# Check memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "\n",
    "# We can free some memory by converting \"strings\" to categorical\n",
    "for col in index_columns:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Print memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Release date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release week\n",
      "    Original grid_df:   1.8GiB\n",
      "     Reduced grid_df:   1.5GiB\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Leading zero values does not mean zero sales but missing items. \n",
    "We can remove them to save memory.\n",
    "'''\n",
    "\n",
    "release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "release_df.columns = ['store_id','item_id','release']\n",
    "\n",
    "# Merge release_df\n",
    "grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\n",
    "del release_df\n",
    "\n",
    "# Merge calendar_df\n",
    "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n",
    "                      \n",
    "# Now we can cutoff some rows and save memory \n",
    "grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Print original memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# Transform release date\n",
    "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "# Print reduced memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Part 1\n",
      "Size: (47735397, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Save Part 1')\n",
    "\n",
    "# Save to pickle file for future use\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "print('Size:', grid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices\n"
     ]
    }
   ],
   "source": [
    "# Basic aggregations\n",
    "prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# Price normalization (min/max scaling)\n",
    "prices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']\n",
    "\n",
    "# Some items can be inflation dependent and some are not\n",
    "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "# Rolling aggregations with months and years as window\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "# Price momentum by month and year\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "\n",
    "del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Prices and Save Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge prices and save part 2\n",
      "Mem. usage decreased to 1822.44 Mb (62.2% reduction)\n",
      "Size: (47735397, 13)\n"
     ]
    }
   ],
   "source": [
    "# Merge Prices\n",
    "original_columns = list(grid_df)\n",
    "grid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "grid_df = grid_df[MAIN_INDEX+keep_columns]\n",
    "grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "# Save Part 2\n",
    "grid_df.to_pickle('grid_part_2.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "del prices_df\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Calender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = grid_df[MAIN_INDEX]\n",
    "\n",
    "# Merge calendar partly\n",
    "icols = ['date',\n",
    "         'd',\n",
    "         'event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "\n",
    "grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "\n",
    "# Minify data\n",
    "icols = ['event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "for col in icols:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Convert to DateTime\n",
    "grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "# Feature generation from date \n",
    "grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n",
    "grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
    "\n",
    "grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n",
    "\n",
    "# Remove date\n",
    "del grid_df['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Part 3 (Dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save part 3\n",
      "Size: (47735397, 16)\n"
     ]
    }
   ],
   "source": [
    "# Save Part 3\n",
    "grid_df.to_pickle('grid_part_3.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "del calendar_df\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1\n",
    "# Convert 'd' to int\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "del grid_df['wm_yr_wk']\n",
    "grid_df.to_pickle('grid_part_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final List of Fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47735397 entries, 0 to 47735396\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Dtype   \n",
      "---  ------    -----   \n",
      " 0   id        category\n",
      " 1   item_id   category\n",
      " 2   dept_id   category\n",
      " 3   cat_id    category\n",
      " 4   store_id  category\n",
      " 5   state_id  category\n",
      " 6   d         int16   \n",
      " 7   sales     float64 \n",
      " 8   release   int16   \n",
      "dtypes: category(6), float64(1), int16(2)\n",
      "memory usage: 910.7 MB\n"
     ]
    }
   ],
   "source": [
    "grid_df.info()\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_008_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_009_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_010_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_012_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_015_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735392</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735393</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735394</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735395</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735396</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47735397 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     d  sales\n",
       "0         HOBBIES_1_008_CA_1_evaluation     1   12.0\n",
       "1         HOBBIES_1_009_CA_1_evaluation     1    2.0\n",
       "2         HOBBIES_1_010_CA_1_evaluation     1    0.0\n",
       "3         HOBBIES_1_012_CA_1_evaluation     1    0.0\n",
       "4         HOBBIES_1_015_CA_1_evaluation     1    4.0\n",
       "...                                 ...   ...    ...\n",
       "47735392    FOODS_3_823_WI_3_evaluation  1969    NaN\n",
       "47735393    FOODS_3_824_WI_3_evaluation  1969    NaN\n",
       "47735394    FOODS_3_825_WI_3_evaluation  1969    NaN\n",
       "47735395    FOODS_3_826_WI_3_evaluation  1969    NaN\n",
       "47735396    FOODS_3_827_WI_3_evaluation  1969    NaN\n",
       "\n",
       "[47735397 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read grid from https://www.kaggle.com/kyakovlev/m5-simple-fe to make sure that our grids are aligned by index\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "\n",
    "# We only need 'id','d','sales'to make lags and rollings\n",
    "grid_df = grid_df[['id','d','sales']]\n",
    "SHIFT_DAY = 28\n",
    "grid_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create lags\n",
      "6.32 min: Lags\n",
      "Create rolling aggs\n",
      "Rolling period: 7\n",
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 180\n",
      "Shifting period: 1\n",
      "Shifting period: 7\n",
      "Shifting period: 14\n",
      "14.77 min: Lags\n"
     ]
    }
   ],
   "source": [
    "# With 28 day shift\n",
    "start_time = time.time()\n",
    "print('Create lags')\n",
    "\n",
    "LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\n",
    "grid_df = grid_df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in LAG_DAYS\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "\n",
    "\n",
    "# Minify lag columns\n",
    "for col in list(grid_df):\n",
    "    if 'lag' in col:\n",
    "        grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
    "\n",
    "# Rollings with 28 day shift\n",
    "start_time = time.time()\n",
    "print('Create rolling aggs')\n",
    "\n",
    "for i in [7,14,30,60,180]:\n",
    "    print('Rolling period:', i)\n",
    "    grid_df['rolling_mean_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n",
    "    grid_df['rolling_std_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n",
    "\n",
    "# Rollings with sliding shift\n",
    "for d_shift in [1,7,14]: \n",
    "    print('Shifting period:', d_shift)\n",
    "    for d_window in [7,14,30,60]:\n",
    "        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n",
    "        grid_df[col_name] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n",
    "    \n",
    "    \n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save lags and rollings\n"
     ]
    }
   ],
   "source": [
    "print('Save lags and rollings')\n",
    "grid_df.to_pickle('lags_df_'+str(SHIFT_DAY)+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final List of New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47735397 entries, 0 to 47735396\n",
      "Data columns (total 40 columns):\n",
      " #   Column                  Dtype   \n",
      "---  ------                  -----   \n",
      " 0   id                      category\n",
      " 1   d                       int16   \n",
      " 2   sales                   float64 \n",
      " 3   sales_lag_28            float16 \n",
      " 4   sales_lag_29            float16 \n",
      " 5   sales_lag_30            float16 \n",
      " 6   sales_lag_31            float16 \n",
      " 7   sales_lag_32            float16 \n",
      " 8   sales_lag_33            float16 \n",
      " 9   sales_lag_34            float16 \n",
      " 10  sales_lag_35            float16 \n",
      " 11  sales_lag_36            float16 \n",
      " 12  sales_lag_37            float16 \n",
      " 13  sales_lag_38            float16 \n",
      " 14  sales_lag_39            float16 \n",
      " 15  sales_lag_40            float16 \n",
      " 16  sales_lag_41            float16 \n",
      " 17  sales_lag_42            float16 \n",
      " 18  rolling_mean_7          float16 \n",
      " 19  rolling_std_7           float16 \n",
      " 20  rolling_mean_14         float16 \n",
      " 21  rolling_std_14          float16 \n",
      " 22  rolling_mean_30         float16 \n",
      " 23  rolling_std_30          float16 \n",
      " 24  rolling_mean_60         float16 \n",
      " 25  rolling_std_60          float16 \n",
      " 26  rolling_mean_180        float16 \n",
      " 27  rolling_std_180         float16 \n",
      " 28  rolling_mean_tmp_1_7    float16 \n",
      " 29  rolling_mean_tmp_1_14   float16 \n",
      " 30  rolling_mean_tmp_1_30   float16 \n",
      " 31  rolling_mean_tmp_1_60   float16 \n",
      " 32  rolling_mean_tmp_7_7    float16 \n",
      " 33  rolling_mean_tmp_7_14   float16 \n",
      " 34  rolling_mean_tmp_7_30   float16 \n",
      " 35  rolling_mean_tmp_7_60   float16 \n",
      " 36  rolling_mean_tmp_14_7   float16 \n",
      " 37  rolling_mean_tmp_14_14  float16 \n",
      " 38  rolling_mean_tmp_14_30  float16 \n",
      " 39  rolling_mean_tmp_14_60  float16 \n",
      "dtypes: category(1), float16(37), float64(1), int16(1)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['state_id']\n",
      "Encoding ['store_id']\n",
      "Encoding ['cat_id']\n",
      "Encoding ['dept_id']\n",
      "Encoding ['state_id', 'cat_id']\n",
      "Encoding ['state_id', 'dept_id']\n",
      "Encoding ['store_id', 'cat_id']\n",
      "Encoding ['store_id', 'dept_id']\n",
      "Encoding ['item_id']\n",
      "Encoding ['item_id', 'state_id']\n",
      "Encoding ['item_id', 'store_id']\n"
     ]
    }
   ],
   "source": [
    "# Read grid from https://www.kaggle.com/kyakovlev/m5-simple-fe to make sure that our grids are aligned by index\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df[TARGET][grid_df['d']>(END_TRAIN-28)] = np.nan ##************need to check\n",
    "base_cols = list(grid_df)\n",
    "\n",
    "icols =  [\n",
    "            ['state_id'],\n",
    "            ['store_id'],\n",
    "            ['cat_id'],\n",
    "            ['dept_id'],\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "            ]\n",
    "\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
    "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
    "\n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "grid_df = grid_df[['id','d']+keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Mean/Std encoding\n"
     ]
    }
   ],
   "source": [
    "print('Save Mean/Std encoding')\n",
    "grid_df.to_pickle('mean_encoding_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47735397 entries, 0 to 47735396\n",
      "Data columns (total 24 columns):\n",
      " #   Column                     Dtype   \n",
      "---  ------                     -----   \n",
      " 0   id                         category\n",
      " 1   d                          int16   \n",
      " 2   enc_state_id_mean          float16 \n",
      " 3   enc_state_id_std           float16 \n",
      " 4   enc_store_id_mean          float16 \n",
      " 5   enc_store_id_std           float16 \n",
      " 6   enc_cat_id_mean            float16 \n",
      " 7   enc_cat_id_std             float16 \n",
      " 8   enc_dept_id_mean           float16 \n",
      " 9   enc_dept_id_std            float16 \n",
      " 10  enc_state_id_cat_id_mean   float16 \n",
      " 11  enc_state_id_cat_id_std    float16 \n",
      " 12  enc_state_id_dept_id_mean  float16 \n",
      " 13  enc_state_id_dept_id_std   float16 \n",
      " 14  enc_store_id_cat_id_mean   float16 \n",
      " 15  enc_store_id_cat_id_std    float16 \n",
      " 16  enc_store_id_dept_id_mean  float16 \n",
      " 17  enc_store_id_dept_id_std   float16 \n",
      " 18  enc_item_id_mean           float16 \n",
      " 19  enc_item_id_std            float16 \n",
      " 20  enc_item_id_state_id_mean  float16 \n",
      " 21  enc_item_id_state_id_std   float16 \n",
      " 22  enc_item_id_store_id_mean  float16 \n",
      " 23  enc_item_id_store_id_std   float16 \n",
      "dtypes: category(1), float16(22), int16(1)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
