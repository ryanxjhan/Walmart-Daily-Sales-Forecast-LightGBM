{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TYPE = 'validation'\n",
    "TYPE = 'evaluation'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "# multiprocess runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "\n",
    "    # With memory limits we have to read \n",
    "    # lags and mean encoding features\n",
    "    # separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned \n",
    "    # we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "# helper to make dynamic rolling lags\n",
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(LAG_DAY):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 1                          # Our model version\n",
    "SEED = 42                        # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "\n",
    "if TYPE == 'validmyself':\n",
    "    END_TRAIN = 1913-28 \n",
    "elif TYPE == 'validation':\n",
    "    END_TRAIN = 1913\n",
    "elif TYPE == 'evaluation':\n",
    "    END_TRAIN = 1913+28\n",
    "else:\n",
    "    print('WRONG!!!')    # Last day in train set\n",
    "    \n",
    "    \n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "USE_AUX     = False               # Use or not pretrained models\n",
    "\n",
    "#FEATURES to remove\n",
    "## These features lead to overfit\n",
    "## or values not present in test set\n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d',TARGET]\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "#PATHS for Features\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "BASE     = 'grid_part_1.pkl'\n",
    "PRICE    = 'grid_part_2.pkl'\n",
    "CALENDAR = 'grid_part_3.pkl'\n",
    "LAGS     = 'lags_df_28.pkl'\n",
    "MEAN_ENC = 'mean_encoding_df.pkl'\n",
    "\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "AUX_MODELS = '../input/m5-aux-models/'\n",
    "\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1\n",
      "[100]\tvalid_0's rmse: 2.01638\n",
      "[200]\tvalid_0's rmse: 1.98408\n",
      "[300]\tvalid_0's rmse: 1.97417\n",
      "[400]\tvalid_0's rmse: 1.96781\n",
      "[500]\tvalid_0's rmse: 1.96263\n",
      "[600]\tvalid_0's rmse: 1.95721\n",
      "[700]\tvalid_0's rmse: 1.95204\n",
      "[800]\tvalid_0's rmse: 1.94672\n",
      "[900]\tvalid_0's rmse: 1.94219\n",
      "[1000]\tvalid_0's rmse: 1.93713\n",
      "[1100]\tvalid_0's rmse: 1.93219\n",
      "[1200]\tvalid_0's rmse: 1.92775\n",
      "[1300]\tvalid_0's rmse: 1.92265\n",
      "[1400]\tvalid_0's rmse: 1.91802\n",
      "Train CA_2\n",
      "[100]\tvalid_0's rmse: 1.94672\n",
      "[200]\tvalid_0's rmse: 1.88961\n",
      "[300]\tvalid_0's rmse: 1.87449\n",
      "[400]\tvalid_0's rmse: 1.86686\n",
      "[500]\tvalid_0's rmse: 1.86041\n",
      "[600]\tvalid_0's rmse: 1.85396\n",
      "[700]\tvalid_0's rmse: 1.84797\n",
      "[800]\tvalid_0's rmse: 1.84255\n",
      "[900]\tvalid_0's rmse: 1.83811\n",
      "[1000]\tvalid_0's rmse: 1.83284\n",
      "[1100]\tvalid_0's rmse: 1.82821\n",
      "[1200]\tvalid_0's rmse: 1.82388\n",
      "[1300]\tvalid_0's rmse: 1.81953\n",
      "[1400]\tvalid_0's rmse: 1.8152\n",
      "Train CA_3\n",
      "[100]\tvalid_0's rmse: 2.38892\n",
      "[200]\tvalid_0's rmse: 2.34715\n",
      "[300]\tvalid_0's rmse: 2.33069\n",
      "[400]\tvalid_0's rmse: 2.32\n",
      "[500]\tvalid_0's rmse: 2.31318\n",
      "[600]\tvalid_0's rmse: 2.30513\n",
      "[700]\tvalid_0's rmse: 2.29839\n",
      "[800]\tvalid_0's rmse: 2.29191\n",
      "[900]\tvalid_0's rmse: 2.28678\n",
      "[1000]\tvalid_0's rmse: 2.28158\n",
      "[1100]\tvalid_0's rmse: 2.27652\n",
      "[1200]\tvalid_0's rmse: 2.27075\n",
      "[1300]\tvalid_0's rmse: 2.26613\n",
      "[1400]\tvalid_0's rmse: 2.26096\n",
      "Train CA_4\n",
      "[100]\tvalid_0's rmse: 1.39197\n",
      "[200]\tvalid_0's rmse: 1.38205\n",
      "[300]\tvalid_0's rmse: 1.37634\n",
      "[400]\tvalid_0's rmse: 1.37246\n",
      "[500]\tvalid_0's rmse: 1.36914\n",
      "[600]\tvalid_0's rmse: 1.36603\n",
      "[700]\tvalid_0's rmse: 1.36275\n",
      "[800]\tvalid_0's rmse: 1.35938\n",
      "[900]\tvalid_0's rmse: 1.3563\n",
      "[1000]\tvalid_0's rmse: 1.35322\n",
      "[1100]\tvalid_0's rmse: 1.35041\n",
      "[1200]\tvalid_0's rmse: 1.34769\n",
      "[1300]\tvalid_0's rmse: 1.3451\n",
      "[1400]\tvalid_0's rmse: 1.34227\n",
      "Train TX_1\n",
      "[100]\tvalid_0's rmse: 1.6149\n",
      "[200]\tvalid_0's rmse: 1.58379\n",
      "[300]\tvalid_0's rmse: 1.57195\n",
      "[400]\tvalid_0's rmse: 1.56706\n",
      "[500]\tvalid_0's rmse: 1.56222\n",
      "[600]\tvalid_0's rmse: 1.55791\n",
      "[700]\tvalid_0's rmse: 1.55383\n",
      "[800]\tvalid_0's rmse: 1.54979\n",
      "[900]\tvalid_0's rmse: 1.54593\n",
      "[1000]\tvalid_0's rmse: 1.54278\n",
      "[1100]\tvalid_0's rmse: 1.53919\n",
      "[1200]\tvalid_0's rmse: 1.53546\n",
      "[1300]\tvalid_0's rmse: 1.53173\n",
      "[1400]\tvalid_0's rmse: 1.52766\n",
      "Train TX_2\n",
      "[100]\tvalid_0's rmse: 1.77921\n",
      "[200]\tvalid_0's rmse: 1.75092\n",
      "[300]\tvalid_0's rmse: 1.74106\n",
      "[400]\tvalid_0's rmse: 1.735\n",
      "[500]\tvalid_0's rmse: 1.72981\n",
      "[600]\tvalid_0's rmse: 1.72506\n",
      "[700]\tvalid_0's rmse: 1.72155\n",
      "[800]\tvalid_0's rmse: 1.71648\n",
      "[900]\tvalid_0's rmse: 1.71176\n",
      "[1000]\tvalid_0's rmse: 1.70845\n",
      "[1100]\tvalid_0's rmse: 1.70485\n",
      "[1200]\tvalid_0's rmse: 1.70141\n",
      "[1300]\tvalid_0's rmse: 1.69727\n",
      "[1400]\tvalid_0's rmse: 1.694\n",
      "Train TX_3\n",
      "[100]\tvalid_0's rmse: 1.86328\n",
      "[200]\tvalid_0's rmse: 1.81676\n",
      "[300]\tvalid_0's rmse: 1.80326\n",
      "[400]\tvalid_0's rmse: 1.79511\n",
      "[500]\tvalid_0's rmse: 1.78884\n",
      "[600]\tvalid_0's rmse: 1.78333\n",
      "[700]\tvalid_0's rmse: 1.77759\n",
      "[800]\tvalid_0's rmse: 1.77208\n",
      "[900]\tvalid_0's rmse: 1.76691\n",
      "[1000]\tvalid_0's rmse: 1.76299\n",
      "[1100]\tvalid_0's rmse: 1.758\n",
      "[1200]\tvalid_0's rmse: 1.75318\n",
      "[1300]\tvalid_0's rmse: 1.7489\n",
      "[1400]\tvalid_0's rmse: 1.74353\n",
      "Train WI_1\n",
      "[100]\tvalid_0's rmse: 1.60707\n",
      "[200]\tvalid_0's rmse: 1.58225\n",
      "[300]\tvalid_0's rmse: 1.57455\n",
      "[400]\tvalid_0's rmse: 1.56893\n",
      "[500]\tvalid_0's rmse: 1.5641\n",
      "[600]\tvalid_0's rmse: 1.55957\n",
      "[700]\tvalid_0's rmse: 1.555\n",
      "[800]\tvalid_0's rmse: 1.55116\n",
      "[900]\tvalid_0's rmse: 1.54729\n",
      "[1000]\tvalid_0's rmse: 1.5438\n",
      "[1100]\tvalid_0's rmse: 1.54016\n",
      "[1200]\tvalid_0's rmse: 1.53697\n",
      "[1300]\tvalid_0's rmse: 1.53391\n",
      "[1400]\tvalid_0's rmse: 1.53059\n",
      "Train WI_2\n",
      "[100]\tvalid_0's rmse: 2.7216\n",
      "[200]\tvalid_0's rmse: 2.62938\n",
      "[300]\tvalid_0's rmse: 2.60163\n",
      "[400]\tvalid_0's rmse: 2.58885\n",
      "[500]\tvalid_0's rmse: 2.57824\n",
      "[600]\tvalid_0's rmse: 2.56945\n",
      "[700]\tvalid_0's rmse: 2.55994\n",
      "[800]\tvalid_0's rmse: 2.54959\n",
      "[900]\tvalid_0's rmse: 2.54082\n",
      "[1000]\tvalid_0's rmse: 2.5296\n",
      "[1100]\tvalid_0's rmse: 2.52231\n",
      "[1200]\tvalid_0's rmse: 2.51467\n",
      "[1300]\tvalid_0's rmse: 2.5069\n",
      "[1400]\tvalid_0's rmse: 2.49973\n",
      "Train WI_3\n",
      "[100]\tvalid_0's rmse: 1.92119\n",
      "[200]\tvalid_0's rmse: 1.87075\n",
      "[300]\tvalid_0's rmse: 1.85549\n",
      "[400]\tvalid_0's rmse: 1.8473\n",
      "[500]\tvalid_0's rmse: 1.83976\n",
      "[600]\tvalid_0's rmse: 1.8334\n",
      "[700]\tvalid_0's rmse: 1.82656\n",
      "[800]\tvalid_0's rmse: 1.82083\n",
      "[900]\tvalid_0's rmse: 1.81545\n",
      "[1000]\tvalid_0's rmse: 1.80963\n",
      "[1100]\tvalid_0's rmse: 1.80441\n",
      "[1200]\tvalid_0's rmse: 1.80059\n",
      "[1300]\tvalid_0's rmse: 1.79511\n",
      "[1400]\tvalid_0's rmse: 1.79028\n"
     ]
    }
   ],
   "source": [
    "for store_id in STORES_IDS:\n",
    "    print('Train', store_id)\n",
    "    \n",
    "    # Get grid for current store\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "    \n",
    "    # Masks for \n",
    "    # Train (All data less than 1913)\n",
    "    # \"Validation\" (Last 28 days - not real validatio set)\n",
    "    # Test (All data greater than 1913 day, \n",
    "    #       with some gap for recursive features)\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "    \n",
    "    # Apply masks and save lgb dataset as bin\n",
    "    # to reduce memory spikes during dtype convertations\n",
    "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
    "    # \"To avoid any conversions, you should always use np.float32\"\n",
    "    # or save to bin before start training\n",
    "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                       label=grid_df[train_mask][TARGET])\n",
    "    train_data.save_binary('train_data.bin')\n",
    "    train_data = lgb.Dataset('train_data.bin')\n",
    "    \n",
    "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                       label=grid_df[valid_mask][TARGET])\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle('test_'+store_id+'.pkl')\n",
    "    del grid_df\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "    \n",
    "    # Save model - it's not real '.bin' but a pickle file\n",
    "    # estimator = lgb.Booster(model_file='model.txt')\n",
    "    # can only predict with the best iteration (or the saving iteration)\n",
    "    # pickle.dump gives us more flexibility\n",
    "    # like estimator.predict(TEST, num_iteration=100)\n",
    "    # num_iteration - number of iteration want to predict with, \n",
    "    # NULL or <= 0 means use best iteration\n",
    "    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "    # Remove temporary files and objects \n",
    "    # to free some hdd space and ram memory\n",
    "    !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict | Day: 1\n",
      "##########  0.46 min round |  0.46 min total |  39824.51 day sales |\n",
      "Predict | Day: 2\n",
      "##########  0.46 min round |  0.91 min total |  37020.80 day sales |\n",
      "Predict | Day: 3\n",
      "##########  0.46 min round |  1.37 min total |  37048.21 day sales |\n",
      "Predict | Day: 4\n",
      "##########  0.46 min round |  1.83 min total |  37113.79 day sales |\n",
      "Predict | Day: 5\n",
      "##########  0.46 min round |  2.28 min total |  42103.53 day sales |\n",
      "Predict | Day: 6\n",
      "##########  0.46 min round |  2.74 min total |  50288.65 day sales |\n",
      "Predict | Day: 7\n",
      "##########  0.46 min round |  3.20 min total |  51139.13 day sales |\n",
      "Predict | Day: 8\n",
      "##########  0.45 min round |  3.65 min total |  45090.29 day sales |\n",
      "Predict | Day: 9\n",
      "##########  0.46 min round |  4.11 min total |  39057.84 day sales |\n",
      "Predict | Day: 10\n",
      "##########  0.46 min round |  4.56 min total |  44161.55 day sales |\n",
      "Predict | Day: 11\n",
      "##########  0.45 min round |  5.02 min total |  45257.33 day sales |\n",
      "Predict | Day: 12\n",
      "##########  0.46 min round |  5.47 min total |  53102.28 day sales |\n",
      "Predict | Day: 13\n",
      "##########  0.46 min round |  5.93 min total |  55773.18 day sales |\n",
      "Predict | Day: 14\n",
      "##########  0.46 min round |  6.39 min total |  57914.31 day sales |\n",
      "Predict | Day: 15\n",
      "##########  0.46 min round |  6.85 min total |  47902.66 day sales |\n",
      "Predict | Day: 16\n",
      "##########  0.45 min round |  7.30 min total |  43562.56 day sales |\n",
      "Predict | Day: 17\n",
      "##########  0.46 min round |  7.75 min total |  42922.50 day sales |\n",
      "Predict | Day: 18\n",
      "##########  0.46 min round |  8.21 min total |  44925.37 day sales |\n",
      "Predict | Day: 19\n",
      "##########  0.46 min round |  8.67 min total |  46824.72 day sales |\n",
      "Predict | Day: 20\n",
      "##########  0.46 min round |  9.13 min total |  57808.97 day sales |\n",
      "Predict | Day: 21\n",
      "##########  0.46 min round |  9.59 min total |  59190.22 day sales |\n",
      "Predict | Day: 22\n",
      "##########  0.46 min round |  10.05 min total |  46442.34 day sales |\n",
      "Predict | Day: 23\n",
      "##########  0.46 min round |  10.50 min total |  43387.22 day sales |\n",
      "Predict | Day: 24\n",
      "##########  0.46 min round |  10.96 min total |  44916.39 day sales |\n",
      "Predict | Day: 25\n",
      "##########  0.46 min round |  11.42 min total |  41480.15 day sales |\n",
      "Predict | Day: 26\n",
      "##########  0.46 min round |  11.88 min total |  45725.52 day sales |\n",
      "Predict | Day: 27\n",
      "##########  0.46 min round |  12.34 min total |  54351.36 day sales |\n",
      "Predict | Day: 28\n",
      "##########  0.46 min round |  12.80 min total |  49981.02 day sales |\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>1.016019</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.863089</td>\n",
       "      <td>0.860954</td>\n",
       "      <td>1.060584</td>\n",
       "      <td>1.325702</td>\n",
       "      <td>1.143586</td>\n",
       "      <td>1.153598</td>\n",
       "      <td>0.927167</td>\n",
       "      <td>...</td>\n",
       "      <td>1.081411</td>\n",
       "      <td>1.452636</td>\n",
       "      <td>1.212648</td>\n",
       "      <td>1.036319</td>\n",
       "      <td>0.892346</td>\n",
       "      <td>0.917212</td>\n",
       "      <td>0.983924</td>\n",
       "      <td>1.222466</td>\n",
       "      <td>1.336801</td>\n",
       "      <td>1.096542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>0.209665</td>\n",
       "      <td>0.202462</td>\n",
       "      <td>0.201996</td>\n",
       "      <td>0.195878</td>\n",
       "      <td>0.231340</td>\n",
       "      <td>0.288717</td>\n",
       "      <td>0.318061</td>\n",
       "      <td>0.217098</td>\n",
       "      <td>0.190154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236810</td>\n",
       "      <td>0.289340</td>\n",
       "      <td>0.336094</td>\n",
       "      <td>0.206175</td>\n",
       "      <td>0.210754</td>\n",
       "      <td>0.214495</td>\n",
       "      <td>0.224534</td>\n",
       "      <td>0.263945</td>\n",
       "      <td>0.346883</td>\n",
       "      <td>0.397126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>0.532586</td>\n",
       "      <td>0.495946</td>\n",
       "      <td>0.497307</td>\n",
       "      <td>0.508057</td>\n",
       "      <td>0.666772</td>\n",
       "      <td>0.794551</td>\n",
       "      <td>0.853137</td>\n",
       "      <td>0.478137</td>\n",
       "      <td>0.494207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637718</td>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.682010</td>\n",
       "      <td>0.507345</td>\n",
       "      <td>0.437814</td>\n",
       "      <td>0.487484</td>\n",
       "      <td>0.490248</td>\n",
       "      <td>0.659896</td>\n",
       "      <td>0.680418</td>\n",
       "      <td>0.721003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>1.508991</td>\n",
       "      <td>1.329832</td>\n",
       "      <td>1.337885</td>\n",
       "      <td>1.436322</td>\n",
       "      <td>1.893035</td>\n",
       "      <td>2.634785</td>\n",
       "      <td>2.909381</td>\n",
       "      <td>1.992759</td>\n",
       "      <td>1.411556</td>\n",
       "      <td>...</td>\n",
       "      <td>1.895158</td>\n",
       "      <td>2.644562</td>\n",
       "      <td>2.972997</td>\n",
       "      <td>1.770339</td>\n",
       "      <td>1.413260</td>\n",
       "      <td>1.470978</td>\n",
       "      <td>1.483530</td>\n",
       "      <td>1.891046</td>\n",
       "      <td>2.566123</td>\n",
       "      <td>2.627022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>0.933289</td>\n",
       "      <td>0.902085</td>\n",
       "      <td>0.926273</td>\n",
       "      <td>0.984511</td>\n",
       "      <td>1.021761</td>\n",
       "      <td>1.299919</td>\n",
       "      <td>1.382790</td>\n",
       "      <td>1.104397</td>\n",
       "      <td>1.005090</td>\n",
       "      <td>...</td>\n",
       "      <td>1.174641</td>\n",
       "      <td>1.503533</td>\n",
       "      <td>1.440640</td>\n",
       "      <td>1.119657</td>\n",
       "      <td>0.962330</td>\n",
       "      <td>1.026712</td>\n",
       "      <td>1.135623</td>\n",
       "      <td>1.382640</td>\n",
       "      <td>1.620631</td>\n",
       "      <td>1.368177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>0.457488</td>\n",
       "      <td>0.422124</td>\n",
       "      <td>0.407579</td>\n",
       "      <td>0.456975</td>\n",
       "      <td>0.494427</td>\n",
       "      <td>0.519157</td>\n",
       "      <td>0.631172</td>\n",
       "      <td>0.484014</td>\n",
       "      <td>0.500971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555696</td>\n",
       "      <td>0.715410</td>\n",
       "      <td>0.850520</td>\n",
       "      <td>0.584696</td>\n",
       "      <td>0.611093</td>\n",
       "      <td>0.583971</td>\n",
       "      <td>0.463133</td>\n",
       "      <td>0.470809</td>\n",
       "      <td>0.567758</td>\n",
       "      <td>0.643348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>0.269612</td>\n",
       "      <td>0.269652</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.208012</td>\n",
       "      <td>0.224886</td>\n",
       "      <td>0.275140</td>\n",
       "      <td>0.283919</td>\n",
       "      <td>0.280258</td>\n",
       "      <td>0.254734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282436</td>\n",
       "      <td>0.361154</td>\n",
       "      <td>0.420467</td>\n",
       "      <td>0.337720</td>\n",
       "      <td>0.388915</td>\n",
       "      <td>0.420287</td>\n",
       "      <td>0.297426</td>\n",
       "      <td>0.257637</td>\n",
       "      <td>0.318984</td>\n",
       "      <td>0.346908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>0.627418</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>0.466312</td>\n",
       "      <td>0.437865</td>\n",
       "      <td>0.496810</td>\n",
       "      <td>0.599151</td>\n",
       "      <td>0.648246</td>\n",
       "      <td>0.585364</td>\n",
       "      <td>0.501346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843144</td>\n",
       "      <td>1.225290</td>\n",
       "      <td>1.355973</td>\n",
       "      <td>1.011092</td>\n",
       "      <td>1.054675</td>\n",
       "      <td>1.080341</td>\n",
       "      <td>0.675670</td>\n",
       "      <td>0.677950</td>\n",
       "      <td>0.818597</td>\n",
       "      <td>0.977020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>1.076575</td>\n",
       "      <td>1.065661</td>\n",
       "      <td>1.060419</td>\n",
       "      <td>0.944252</td>\n",
       "      <td>1.032034</td>\n",
       "      <td>1.102889</td>\n",
       "      <td>1.050712</td>\n",
       "      <td>1.240361</td>\n",
       "      <td>1.011412</td>\n",
       "      <td>...</td>\n",
       "      <td>1.112941</td>\n",
       "      <td>1.422140</td>\n",
       "      <td>1.385522</td>\n",
       "      <td>1.288973</td>\n",
       "      <td>1.455521</td>\n",
       "      <td>1.347306</td>\n",
       "      <td>1.038740</td>\n",
       "      <td>1.137330</td>\n",
       "      <td>1.276893</td>\n",
       "      <td>1.357613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>1.596561</td>\n",
       "      <td>1.524541</td>\n",
       "      <td>1.484434</td>\n",
       "      <td>1.527995</td>\n",
       "      <td>1.804479</td>\n",
       "      <td>1.989321</td>\n",
       "      <td>1.851393</td>\n",
       "      <td>1.855292</td>\n",
       "      <td>1.644907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.502808</td>\n",
       "      <td>2.001843</td>\n",
       "      <td>2.012916</td>\n",
       "      <td>1.711010</td>\n",
       "      <td>1.774993</td>\n",
       "      <td>1.509772</td>\n",
       "      <td>1.499247</td>\n",
       "      <td>1.600819</td>\n",
       "      <td>1.766148</td>\n",
       "      <td>1.906364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id        F1        F2        F3        F4  \\\n",
       "0      HOBBIES_1_001_CA_1_evaluation  1.016019  0.820798  0.863089  0.860954   \n",
       "1      HOBBIES_1_002_CA_1_evaluation  0.209665  0.202462  0.201996  0.195878   \n",
       "2      HOBBIES_1_003_CA_1_evaluation  0.532586  0.495946  0.497307  0.508057   \n",
       "3      HOBBIES_1_004_CA_1_evaluation  1.508991  1.329832  1.337885  1.436322   \n",
       "4      HOBBIES_1_005_CA_1_evaluation  0.933289  0.902085  0.926273  0.984511   \n",
       "...                              ...       ...       ...       ...       ...   \n",
       "30485    FOODS_3_823_WI_3_evaluation  0.457488  0.422124  0.407579  0.456975   \n",
       "30486    FOODS_3_824_WI_3_evaluation  0.269612  0.269652  0.226230  0.208012   \n",
       "30487    FOODS_3_825_WI_3_evaluation  0.627418  0.522876  0.466312  0.437865   \n",
       "30488    FOODS_3_826_WI_3_evaluation  1.076575  1.065661  1.060419  0.944252   \n",
       "30489    FOODS_3_827_WI_3_evaluation  1.596561  1.524541  1.484434  1.527995   \n",
       "\n",
       "             F5        F6        F7        F8        F9  ...       F19  \\\n",
       "0      1.060584  1.325702  1.143586  1.153598  0.927167  ...  1.081411   \n",
       "1      0.231340  0.288717  0.318061  0.217098  0.190154  ...  0.236810   \n",
       "2      0.666772  0.794551  0.853137  0.478137  0.494207  ...  0.637718   \n",
       "3      1.893035  2.634785  2.909381  1.992759  1.411556  ...  1.895158   \n",
       "4      1.021761  1.299919  1.382790  1.104397  1.005090  ...  1.174641   \n",
       "...         ...       ...       ...       ...       ...  ...       ...   \n",
       "30485  0.494427  0.519157  0.631172  0.484014  0.500971  ...  0.555696   \n",
       "30486  0.224886  0.275140  0.283919  0.280258  0.254734  ...  0.282436   \n",
       "30487  0.496810  0.599151  0.648246  0.585364  0.501346  ...  0.843144   \n",
       "30488  1.032034  1.102889  1.050712  1.240361  1.011412  ...  1.112941   \n",
       "30489  1.804479  1.989321  1.851393  1.855292  1.644907  ...  1.502808   \n",
       "\n",
       "            F20       F21       F22       F23       F24       F25       F26  \\\n",
       "0      1.452636  1.212648  1.036319  0.892346  0.917212  0.983924  1.222466   \n",
       "1      0.289340  0.336094  0.206175  0.210754  0.214495  0.224534  0.263945   \n",
       "2      0.720339  0.682010  0.507345  0.437814  0.487484  0.490248  0.659896   \n",
       "3      2.644562  2.972997  1.770339  1.413260  1.470978  1.483530  1.891046   \n",
       "4      1.503533  1.440640  1.119657  0.962330  1.026712  1.135623  1.382640   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30485  0.715410  0.850520  0.584696  0.611093  0.583971  0.463133  0.470809   \n",
       "30486  0.361154  0.420467  0.337720  0.388915  0.420287  0.297426  0.257637   \n",
       "30487  1.225290  1.355973  1.011092  1.054675  1.080341  0.675670  0.677950   \n",
       "30488  1.422140  1.385522  1.288973  1.455521  1.347306  1.038740  1.137330   \n",
       "30489  2.001843  2.012916  1.711010  1.774993  1.509772  1.499247  1.600819   \n",
       "\n",
       "            F27       F28  \n",
       "0      1.336801  1.096542  \n",
       "1      0.346883  0.397126  \n",
       "2      0.680418  0.721003  \n",
       "3      2.566123  2.627022  \n",
       "4      1.620631  1.368177  \n",
       "...         ...       ...  \n",
       "30485  0.567758  0.643348  \n",
       "30486  0.318984  0.346908  \n",
       "30487  0.818597  0.977020  \n",
       "30488  1.276893  1.357613  \n",
       "30489  1.766148  1.906364  \n",
       "\n",
       "[30490 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with \n",
    "# a small part of the training data \n",
    "# to make recursive features\n",
    "base_test = get_base_test()\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming\n",
    "# we will calculate it for whole day\n",
    "for PREDICT_DAY in range(1,29):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
    "        \n",
    "    for store_id in STORES_IDS:\n",
    "        \n",
    "        # Read all our models and make predictions\n",
    "        # for each day/store pairs\n",
    "        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
    "        if USE_AUX:\n",
    "            model_path = AUX_MODELS + model_path\n",
    "        \n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
    "        store_mask = base_test['store_id']==store_id\n",
    "        \n",
    "        mask = (day_mask)&(store_mask)\n",
    "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "    \n",
    "    # Make good column naming and add \n",
    "    # to all_preds DataFrame\n",
    "    temp_df = base_test[day_mask][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    del temp_df\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading competition sample submission and\n",
    "# merging our predictions\n",
    "# As we have predictions only for \"_validation\" data\n",
    "# we need to do fillna() for \"_evaluation\" items\n",
    "\n",
    "\n",
    "\n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
    "\n",
    "    \n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv('submission_v'+str(VER)+'_'+TYPE+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>1.016019</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.863089</td>\n",
       "      <td>0.860954</td>\n",
       "      <td>1.060584</td>\n",
       "      <td>1.325702</td>\n",
       "      <td>1.143586</td>\n",
       "      <td>1.153598</td>\n",
       "      <td>0.927167</td>\n",
       "      <td>...</td>\n",
       "      <td>1.081411</td>\n",
       "      <td>1.452636</td>\n",
       "      <td>1.212648</td>\n",
       "      <td>1.036319</td>\n",
       "      <td>0.892346</td>\n",
       "      <td>0.917212</td>\n",
       "      <td>0.983924</td>\n",
       "      <td>1.222466</td>\n",
       "      <td>1.336801</td>\n",
       "      <td>1.096542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>0.209665</td>\n",
       "      <td>0.202462</td>\n",
       "      <td>0.201996</td>\n",
       "      <td>0.195878</td>\n",
       "      <td>0.231340</td>\n",
       "      <td>0.288717</td>\n",
       "      <td>0.318061</td>\n",
       "      <td>0.217098</td>\n",
       "      <td>0.190154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236810</td>\n",
       "      <td>0.289340</td>\n",
       "      <td>0.336094</td>\n",
       "      <td>0.206175</td>\n",
       "      <td>0.210754</td>\n",
       "      <td>0.214495</td>\n",
       "      <td>0.224534</td>\n",
       "      <td>0.263945</td>\n",
       "      <td>0.346883</td>\n",
       "      <td>0.397126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>0.532586</td>\n",
       "      <td>0.495946</td>\n",
       "      <td>0.497307</td>\n",
       "      <td>0.508057</td>\n",
       "      <td>0.666772</td>\n",
       "      <td>0.794551</td>\n",
       "      <td>0.853137</td>\n",
       "      <td>0.478137</td>\n",
       "      <td>0.494207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637718</td>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.682010</td>\n",
       "      <td>0.507345</td>\n",
       "      <td>0.437814</td>\n",
       "      <td>0.487484</td>\n",
       "      <td>0.490248</td>\n",
       "      <td>0.659896</td>\n",
       "      <td>0.680418</td>\n",
       "      <td>0.721003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>1.508991</td>\n",
       "      <td>1.329832</td>\n",
       "      <td>1.337885</td>\n",
       "      <td>1.436322</td>\n",
       "      <td>1.893035</td>\n",
       "      <td>2.634785</td>\n",
       "      <td>2.909381</td>\n",
       "      <td>1.992759</td>\n",
       "      <td>1.411556</td>\n",
       "      <td>...</td>\n",
       "      <td>1.895158</td>\n",
       "      <td>2.644562</td>\n",
       "      <td>2.972997</td>\n",
       "      <td>1.770339</td>\n",
       "      <td>1.413260</td>\n",
       "      <td>1.470978</td>\n",
       "      <td>1.483530</td>\n",
       "      <td>1.891046</td>\n",
       "      <td>2.566123</td>\n",
       "      <td>2.627022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>0.933289</td>\n",
       "      <td>0.902085</td>\n",
       "      <td>0.926273</td>\n",
       "      <td>0.984511</td>\n",
       "      <td>1.021761</td>\n",
       "      <td>1.299919</td>\n",
       "      <td>1.382790</td>\n",
       "      <td>1.104397</td>\n",
       "      <td>1.005090</td>\n",
       "      <td>...</td>\n",
       "      <td>1.174641</td>\n",
       "      <td>1.503533</td>\n",
       "      <td>1.440640</td>\n",
       "      <td>1.119657</td>\n",
       "      <td>0.962330</td>\n",
       "      <td>1.026712</td>\n",
       "      <td>1.135623</td>\n",
       "      <td>1.382640</td>\n",
       "      <td>1.620631</td>\n",
       "      <td>1.368177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>0.457488</td>\n",
       "      <td>0.422124</td>\n",
       "      <td>0.407579</td>\n",
       "      <td>0.456975</td>\n",
       "      <td>0.494427</td>\n",
       "      <td>0.519157</td>\n",
       "      <td>0.631172</td>\n",
       "      <td>0.484014</td>\n",
       "      <td>0.500971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555696</td>\n",
       "      <td>0.715410</td>\n",
       "      <td>0.850520</td>\n",
       "      <td>0.584696</td>\n",
       "      <td>0.611093</td>\n",
       "      <td>0.583971</td>\n",
       "      <td>0.463133</td>\n",
       "      <td>0.470809</td>\n",
       "      <td>0.567758</td>\n",
       "      <td>0.643348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>0.269612</td>\n",
       "      <td>0.269652</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.208012</td>\n",
       "      <td>0.224886</td>\n",
       "      <td>0.275140</td>\n",
       "      <td>0.283919</td>\n",
       "      <td>0.280258</td>\n",
       "      <td>0.254734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282436</td>\n",
       "      <td>0.361154</td>\n",
       "      <td>0.420467</td>\n",
       "      <td>0.337720</td>\n",
       "      <td>0.388915</td>\n",
       "      <td>0.420287</td>\n",
       "      <td>0.297426</td>\n",
       "      <td>0.257637</td>\n",
       "      <td>0.318984</td>\n",
       "      <td>0.346908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>0.627418</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>0.466312</td>\n",
       "      <td>0.437865</td>\n",
       "      <td>0.496810</td>\n",
       "      <td>0.599151</td>\n",
       "      <td>0.648246</td>\n",
       "      <td>0.585364</td>\n",
       "      <td>0.501346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843144</td>\n",
       "      <td>1.225290</td>\n",
       "      <td>1.355973</td>\n",
       "      <td>1.011092</td>\n",
       "      <td>1.054675</td>\n",
       "      <td>1.080341</td>\n",
       "      <td>0.675670</td>\n",
       "      <td>0.677950</td>\n",
       "      <td>0.818597</td>\n",
       "      <td>0.977020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>1.076575</td>\n",
       "      <td>1.065661</td>\n",
       "      <td>1.060419</td>\n",
       "      <td>0.944252</td>\n",
       "      <td>1.032034</td>\n",
       "      <td>1.102889</td>\n",
       "      <td>1.050712</td>\n",
       "      <td>1.240361</td>\n",
       "      <td>1.011412</td>\n",
       "      <td>...</td>\n",
       "      <td>1.112941</td>\n",
       "      <td>1.422140</td>\n",
       "      <td>1.385522</td>\n",
       "      <td>1.288973</td>\n",
       "      <td>1.455521</td>\n",
       "      <td>1.347306</td>\n",
       "      <td>1.038740</td>\n",
       "      <td>1.137330</td>\n",
       "      <td>1.276893</td>\n",
       "      <td>1.357613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>1.596561</td>\n",
       "      <td>1.524541</td>\n",
       "      <td>1.484434</td>\n",
       "      <td>1.527995</td>\n",
       "      <td>1.804479</td>\n",
       "      <td>1.989321</td>\n",
       "      <td>1.851393</td>\n",
       "      <td>1.855292</td>\n",
       "      <td>1.644907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.502808</td>\n",
       "      <td>2.001843</td>\n",
       "      <td>2.012916</td>\n",
       "      <td>1.711010</td>\n",
       "      <td>1.774993</td>\n",
       "      <td>1.509772</td>\n",
       "      <td>1.499247</td>\n",
       "      <td>1.600819</td>\n",
       "      <td>1.766148</td>\n",
       "      <td>1.906364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id        F1        F2        F3        F4  \\\n",
       "0      HOBBIES_1_001_CA_1_evaluation  1.016019  0.820798  0.863089  0.860954   \n",
       "1      HOBBIES_1_002_CA_1_evaluation  0.209665  0.202462  0.201996  0.195878   \n",
       "2      HOBBIES_1_003_CA_1_evaluation  0.532586  0.495946  0.497307  0.508057   \n",
       "3      HOBBIES_1_004_CA_1_evaluation  1.508991  1.329832  1.337885  1.436322   \n",
       "4      HOBBIES_1_005_CA_1_evaluation  0.933289  0.902085  0.926273  0.984511   \n",
       "...                              ...       ...       ...       ...       ...   \n",
       "30485    FOODS_3_823_WI_3_evaluation  0.457488  0.422124  0.407579  0.456975   \n",
       "30486    FOODS_3_824_WI_3_evaluation  0.269612  0.269652  0.226230  0.208012   \n",
       "30487    FOODS_3_825_WI_3_evaluation  0.627418  0.522876  0.466312  0.437865   \n",
       "30488    FOODS_3_826_WI_3_evaluation  1.076575  1.065661  1.060419  0.944252   \n",
       "30489    FOODS_3_827_WI_3_evaluation  1.596561  1.524541  1.484434  1.527995   \n",
       "\n",
       "             F5        F6        F7        F8        F9  ...       F19  \\\n",
       "0      1.060584  1.325702  1.143586  1.153598  0.927167  ...  1.081411   \n",
       "1      0.231340  0.288717  0.318061  0.217098  0.190154  ...  0.236810   \n",
       "2      0.666772  0.794551  0.853137  0.478137  0.494207  ...  0.637718   \n",
       "3      1.893035  2.634785  2.909381  1.992759  1.411556  ...  1.895158   \n",
       "4      1.021761  1.299919  1.382790  1.104397  1.005090  ...  1.174641   \n",
       "...         ...       ...       ...       ...       ...  ...       ...   \n",
       "30485  0.494427  0.519157  0.631172  0.484014  0.500971  ...  0.555696   \n",
       "30486  0.224886  0.275140  0.283919  0.280258  0.254734  ...  0.282436   \n",
       "30487  0.496810  0.599151  0.648246  0.585364  0.501346  ...  0.843144   \n",
       "30488  1.032034  1.102889  1.050712  1.240361  1.011412  ...  1.112941   \n",
       "30489  1.804479  1.989321  1.851393  1.855292  1.644907  ...  1.502808   \n",
       "\n",
       "            F20       F21       F22       F23       F24       F25       F26  \\\n",
       "0      1.452636  1.212648  1.036319  0.892346  0.917212  0.983924  1.222466   \n",
       "1      0.289340  0.336094  0.206175  0.210754  0.214495  0.224534  0.263945   \n",
       "2      0.720339  0.682010  0.507345  0.437814  0.487484  0.490248  0.659896   \n",
       "3      2.644562  2.972997  1.770339  1.413260  1.470978  1.483530  1.891046   \n",
       "4      1.503533  1.440640  1.119657  0.962330  1.026712  1.135623  1.382640   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30485  0.715410  0.850520  0.584696  0.611093  0.583971  0.463133  0.470809   \n",
       "30486  0.361154  0.420467  0.337720  0.388915  0.420287  0.297426  0.257637   \n",
       "30487  1.225290  1.355973  1.011092  1.054675  1.080341  0.675670  0.677950   \n",
       "30488  1.422140  1.385522  1.288973  1.455521  1.347306  1.038740  1.137330   \n",
       "30489  2.001843  2.012916  1.711010  1.774993  1.509772  1.499247  1.600819   \n",
       "\n",
       "            F27       F28  \n",
       "0      1.336801  1.096542  \n",
       "1      0.346883  0.397126  \n",
       "2      0.680418  0.721003  \n",
       "3      2.566123  2.627022  \n",
       "4      1.620631  1.368177  \n",
       "...         ...       ...  \n",
       "30485  0.567758  0.643348  \n",
       "30486  0.318984  0.346908  \n",
       "30487  0.818597  0.977020  \n",
       "30488  1.276893  1.357613  \n",
       "30489  1.766148  1.906364  \n",
       "\n",
       "[30490 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
